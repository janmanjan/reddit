{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61247cd-4203-4e77-8e55-986b0e1ae5cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d6c8ef7-4c3a-4945-bac7-252b560f9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "import regex\n",
    "import emoji\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from skopt import BayesSearchCV\n",
    "from scipy.stats import uniform, loguniform\n",
    "from skopt.space import Integer, Real, Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8779b443-aadc-476b-a139-b61dc8bdf618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv files\n",
    "# title = pd.read_csv('C:/Users/mmoli/GA/projects/project_3/data/title_only_cleaned.csv')\n",
    "# selftext = pd.read_csv('C:/Users/mmoli/GA/projects/project_3/data/selftext_only_cleaned.csv')\n",
    "both = pd.read_csv('C:/Users/mmoli/GA/projects/project_3/data/both_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657761ff-251d-449c-9423-c12546263bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "both.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55f7057-9228-4e3e-bf20-2409554e2660",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tokenizing-Lemmatiziation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41c32ab5-6760-4802-8ac3-da256d2c455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mytokenizing_lem(column):\n",
    "    words = ''\n",
    "    for elements in column:\n",
    "        words += elements\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words_tokens = tokenizer.tokenize(words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_tokens_lem = [lemmatizer.lemmatize(token) for token in words_tokens]\n",
    "    return words_tokens_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33ab5e3d-75d0-4cf2-81ab-fe8f5ec18e39",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What',\n",
       " 's',\n",
       " 'the',\n",
       " 'difference',\n",
       " 'between',\n",
       " 'a',\n",
       " 'dog',\n",
       " 'and',\n",
       " 'a',\n",
       " 'cat',\n",
       " 'lick',\n",
       " 'Why',\n",
       " 'is',\n",
       " 'it',\n",
       " 'wierd',\n",
       " 'to',\n",
       " 'find',\n",
       " 'a',\n",
       " 'latin',\n",
       " 'woman',\n",
       " 'or',\n",
       " 'a',\n",
       " 'Latinas',\n",
       " 'attractive',\n",
       " 'What',\n",
       " 's',\n",
       " 'a',\n",
       " 'healthy',\n",
       " 'way',\n",
       " 'to',\n",
       " 'discipline',\n",
       " 'myself',\n",
       " 'Applying',\n",
       " 'Lube',\n",
       " 'Why',\n",
       " 'don',\n",
       " 't',\n",
       " 'the',\n",
       " 'Democrats',\n",
       " 'make',\n",
       " 'the',\n",
       " 'Republicans',\n",
       " 'ACTUALLY',\n",
       " 'carry',\n",
       " 'out',\n",
       " 'a',\n",
       " 'filibuster',\n",
       " 'When',\n",
       " 'discussing',\n",
       " 'suicide',\n",
       " 'and',\n",
       " 'mental',\n",
       " 'health',\n",
       " 'why',\n",
       " 'doe',\n",
       " 'no',\n",
       " 'one',\n",
       " 'mention',\n",
       " 'that',\n",
       " 'woman',\n",
       " 'attempt',\n",
       " 'suicide',\n",
       " '3x',\n",
       " 's',\n",
       " 'the',\n",
       " 'rate',\n",
       " 'that',\n",
       " 'men',\n",
       " 'do',\n",
       " 'What',\n",
       " 'are',\n",
       " 'the',\n",
       " 'most',\n",
       " 'positive',\n",
       " 'subreddits',\n",
       " 'out',\n",
       " 'there',\n",
       " 'Is',\n",
       " 'it',\n",
       " 'a',\n",
       " 'normal',\n",
       " 'feeling',\n",
       " 'to',\n",
       " 'be',\n",
       " 'confused',\n",
       " 'why',\n",
       " 'someone',\n",
       " 'you',\n",
       " 'find',\n",
       " 'attractive',\n",
       " 'find',\n",
       " 'you',\n",
       " 'attractive',\n",
       " 'How',\n",
       " 'do',\n",
       " 'i',\n",
       " 'make',\n",
       " 'digital',\n",
       " 'estimate',\n",
       " 'of',\n",
       " 'wood',\n",
       " 'project',\n",
       " 'to',\n",
       " 'get',\n",
       " 'accurate',\n",
       " 'measurement',\n",
       " 'Dirt',\n",
       " 'keep',\n",
       " 'getting',\n",
       " 'under',\n",
       " 'my',\n",
       " 'extremely',\n",
       " 'short',\n",
       " 'nail',\n",
       " 'making',\n",
       " 'it',\n",
       " 'painful',\n",
       " 'and',\n",
       " 'difficult',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'clean',\n",
       " 'How',\n",
       " 'can',\n",
       " 'I',\n",
       " 'fix',\n",
       " 'this',\n",
       " 'What',\n",
       " 'Non',\n",
       " 'academic',\n",
       " 'lesson',\n",
       " 'did',\n",
       " 'school',\n",
       " 'drill',\n",
       " 'into',\n",
       " 'you',\n",
       " 'a',\n",
       " 'a',\n",
       " 'kid',\n",
       " 'Forgetting',\n",
       " 'your',\n",
       " 'parent',\n",
       " 'native',\n",
       " 'language',\n",
       " 'Any',\n",
       " 'help',\n",
       " 'advice',\n",
       " 'What',\n",
       " 'do',\n",
       " 'people',\n",
       " 'who',\n",
       " 'are',\n",
       " 'kind',\n",
       " 'but',\n",
       " 'also',\n",
       " 'new',\n",
       " 'to',\n",
       " 'me',\n",
       " 'signing',\n",
       " 'xo',\n",
       " 'or',\n",
       " 'xx',\n",
       " 'in',\n",
       " 'their',\n",
       " 'signature',\n",
       " 'mean',\n",
       " 'What',\n",
       " 'make',\n",
       " 'a',\n",
       " 'conversation',\n",
       " 'deep',\n",
       " 'meaningful',\n",
       " 'to',\n",
       " 'you',\n",
       " 'Do',\n",
       " 'Canadians',\n",
       " 'find',\n",
       " 'eh',\n",
       " 'sorry',\n",
       " 'maple',\n",
       " 'syrup',\n",
       " 'type',\n",
       " 'comment',\n",
       " 'offensive',\n",
       " 'Why',\n",
       " 'do',\n",
       " 'Americans',\n",
       " 'eat',\n",
       " 'so',\n",
       " 'much',\n",
       " 'on',\n",
       " 'Thanksgiving',\n",
       " 'If',\n",
       " 'i',\n",
       " 'stil',\n",
       " 'own',\n",
       " 'my',\n",
       " 'own',\n",
       " 'nude',\n",
       " 'tape',\n",
       " 'from',\n",
       " 'when',\n",
       " 'I',\n",
       " 'wa',\n",
       " 'underage',\n",
       " 'can',\n",
       " 'i',\n",
       " 'be',\n",
       " 'in',\n",
       " 'jail',\n",
       " 'for',\n",
       " 'owning',\n",
       " 'CP',\n",
       " 'Why',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'Ray',\n",
       " 'Lewis',\n",
       " 'get',\n",
       " 'the',\n",
       " 'same',\n",
       " 'treatment',\n",
       " 'a',\n",
       " 'OJ',\n",
       " 'Simpson',\n",
       " 'and',\n",
       " 'or',\n",
       " 'Aaron',\n",
       " 'HernandezIs',\n",
       " 'it',\n",
       " 'actually',\n",
       " 'that',\n",
       " 'unusual',\n",
       " 'that',\n",
       " 'my',\n",
       " 'mom',\n",
       " 'used',\n",
       " 'to',\n",
       " 'use',\n",
       " 'my',\n",
       " 'hair',\n",
       " 'to',\n",
       " 'decorate',\n",
       " 'bread',\n",
       " 'Are',\n",
       " 'autistic',\n",
       " 'people',\n",
       " 'more',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'commit',\n",
       " 'sexual',\n",
       " 'crime',\n",
       " 'Is',\n",
       " 'male',\n",
       " 'sperm',\n",
       " 'considered',\n",
       " 'a',\n",
       " 'seed',\n",
       " 'Is',\n",
       " 'my',\n",
       " 'cpu',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'okay',\n",
       " 'I',\n",
       " 'spilled',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'water',\n",
       " 'on',\n",
       " 'my',\n",
       " 'gaming',\n",
       " 'laptop',\n",
       " '20hrs',\n",
       " 'ago',\n",
       " 'Immediately',\n",
       " 'turned',\n",
       " 'it',\n",
       " 'upside',\n",
       " 'down',\n",
       " 'and',\n",
       " 'wiped',\n",
       " 'it',\n",
       " 'down',\n",
       " 'with',\n",
       " 'fabric',\n",
       " 'It',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'still',\n",
       " 'be',\n",
       " 'running',\n",
       " 'but',\n",
       " 'now',\n",
       " 'every',\n",
       " 'time',\n",
       " 'it',\n",
       " 'stall',\n",
       " 'or',\n",
       " 'anything',\n",
       " 'I',\n",
       " 'm',\n",
       " 'going',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'myself',\n",
       " 'that',\n",
       " 'marvelous',\n",
       " 'Urkellian',\n",
       " 'question',\n",
       " 'Did',\n",
       " 'I',\n",
       " 'do',\n",
       " 'that',\n",
       " 'Why',\n",
       " 'doe',\n",
       " 'it',\n",
       " 'feel',\n",
       " 'weird',\n",
       " 'walking',\n",
       " 'on',\n",
       " 'a',\n",
       " 'broken',\n",
       " 'escalator',\n",
       " 'If',\n",
       " 'I',\n",
       " 'am',\n",
       " 'looking',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'house',\n",
       " 'that',\n",
       " 'is',\n",
       " 'around',\n",
       " '450',\n",
       " '000',\n",
       " 'how',\n",
       " 'much',\n",
       " 'should',\n",
       " 'I',\n",
       " 'put',\n",
       " 'down',\n",
       " 'If',\n",
       " 'I',\n",
       " 'were',\n",
       " 'to',\n",
       " 'somehow',\n",
       " 'pull',\n",
       " 'a',\n",
       " 'firing',\n",
       " 'pin',\n",
       " 'out',\n",
       " 'of',\n",
       " 'a',\n",
       " 'firearm',\n",
       " 'and',\n",
       " 'tap',\n",
       " 'a',\n",
       " 'singular',\n",
       " 'bullet',\n",
       " 'in',\n",
       " 'the',\n",
       " 'place',\n",
       " 'the',\n",
       " 'cause',\n",
       " 'it',\n",
       " 'to',\n",
       " 'ignite',\n",
       " 'could',\n",
       " 'a',\n",
       " 'fire',\n",
       " 'a',\n",
       " 'round',\n",
       " 'without',\n",
       " 'using',\n",
       " 'a',\n",
       " 'gun',\n",
       " 'Response',\n",
       " 'to',\n",
       " 'Getting',\n",
       " 'drink',\n",
       " 'sent',\n",
       " 'to',\n",
       " 'you',\n",
       " 'by',\n",
       " 'guy',\n",
       " 'Was',\n",
       " 'Hitler',\n",
       " 'Jewish',\n",
       " 'What',\n",
       " 'doe',\n",
       " 'Natto',\n",
       " 'or',\n",
       " 'Fermented',\n",
       " 'Soy',\n",
       " 'Beans',\n",
       " 'actually',\n",
       " 'taste',\n",
       " 'like',\n",
       " 'and',\n",
       " 'is',\n",
       " 'it',\n",
       " 'really',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'a',\n",
       " 'people',\n",
       " 'say',\n",
       " 'it',\n",
       " 'is',\n",
       " 'If',\n",
       " 'someone',\n",
       " 'is',\n",
       " 'always',\n",
       " 'changing',\n",
       " 'their',\n",
       " 'profile',\n",
       " 'pic',\n",
       " 'doe',\n",
       " 'that',\n",
       " 'mean',\n",
       " 'their',\n",
       " 'not',\n",
       " 'really',\n",
       " 'secure',\n",
       " 'about',\n",
       " 'how',\n",
       " 'they',\n",
       " 'look',\n",
       " 'How',\n",
       " 'do',\n",
       " 'you',\n",
       " 'express',\n",
       " 'your',\n",
       " 'sexuality',\n",
       " 'Why',\n",
       " 'do',\n",
       " 'you',\n",
       " 'think',\n",
       " 'drug',\n",
       " 'paraphernalia',\n",
       " 'is',\n",
       " 'treated',\n",
       " 'so',\n",
       " 'much',\n",
       " 'more',\n",
       " 'harshly',\n",
       " 'than',\n",
       " 'mere',\n",
       " 'drug',\n",
       " 'possession',\n",
       " 'in',\n",
       " 'Hawaii',\n",
       " 'I',\n",
       " 'am',\n",
       " 'new',\n",
       " 'on',\n",
       " 'reddit',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'not',\n",
       " 'able',\n",
       " 'to',\n",
       " 'post',\n",
       " 'question',\n",
       " 'on',\n",
       " 'many',\n",
       " 'famous',\n",
       " 'subreddit',\n",
       " 'eg',\n",
       " 'r',\n",
       " 'teenager',\n",
       " 'it',\n",
       " 'is',\n",
       " 'showing',\n",
       " 'you',\n",
       " 'have',\n",
       " 'low',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'karma',\n",
       " 'can',\n",
       " 'you',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'what',\n",
       " 'should',\n",
       " 'I',\n",
       " 'do',\n",
       " 'When',\n",
       " 'stating',\n",
       " 'a',\n",
       " 'pronoun',\n",
       " 'why',\n",
       " 'do',\n",
       " 'people',\n",
       " 'feel',\n",
       " 'the',\n",
       " 'need',\n",
       " 'to',\n",
       " 'list',\n",
       " 'it',\n",
       " 'grammatical',\n",
       " 'form',\n",
       " 'Why',\n",
       " 'do',\n",
       " 'people',\n",
       " 'treat',\n",
       " 'people',\n",
       " 'with',\n",
       " 'disability',\n",
       " 'like',\n",
       " 'baby',\n",
       " 'even',\n",
       " 'when',\n",
       " 'they',\n",
       " 'are',\n",
       " 'manager',\n",
       " 'etc',\n",
       " 'If',\n",
       " 'I',\n",
       " 'start',\n",
       " 'dressing',\n",
       " 'super',\n",
       " 'warm',\n",
       " 'in',\n",
       " 'early',\n",
       " 'Winter',\n",
       " 'will',\n",
       " 'I',\n",
       " 'get',\n",
       " 'used',\n",
       " 'to',\n",
       " 'the',\n",
       " 'clothes',\n",
       " 'and',\n",
       " 'feel',\n",
       " 'colder',\n",
       " 'in',\n",
       " 'late',\n",
       " 'Winter',\n",
       " 'Am',\n",
       " 'I',\n",
       " 'ci',\n",
       " 'or',\n",
       " 'not',\n",
       " 'Do',\n",
       " 'trailer',\n",
       " 'generally',\n",
       " 'come',\n",
       " 'with',\n",
       " 'invertersWhy',\n",
       " 'do',\n",
       " 'human',\n",
       " 'get',\n",
       " 'fat',\n",
       " 'after',\n",
       " 'pregnancy',\n",
       " 'but',\n",
       " 'animal',\n",
       " 'don',\n",
       " 't',\n",
       " 'Where',\n",
       " 'can',\n",
       " 'I',\n",
       " 'find',\n",
       " 'a',\n",
       " 'country',\n",
       " 'ethnic',\n",
       " 'eggsIs',\n",
       " 'there',\n",
       " 'such',\n",
       " 'thing',\n",
       " 'a',\n",
       " 'a',\n",
       " 'good',\n",
       " 'dick',\n",
       " 'pic',\n",
       " 'If',\n",
       " 'so',\n",
       " 'how',\n",
       " 'do',\n",
       " 'you',\n",
       " 'do',\n",
       " 'such',\n",
       " 'a',\n",
       " 'thing',\n",
       " 'Preparing',\n",
       " 'for',\n",
       " 'a',\n",
       " 'big',\n",
       " 'move',\n",
       " 'out',\n",
       " 'of',\n",
       " 'state',\n",
       " 'how',\n",
       " 'do',\n",
       " 'adult',\n",
       " 'make',\n",
       " 'new',\n",
       " 'friend',\n",
       " 'Does',\n",
       " 'anyone',\n",
       " 'else',\n",
       " 'feel',\n",
       " 'an',\n",
       " 'overwhelming',\n",
       " 'urge',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'tennis',\n",
       " 'ball',\n",
       " 'when',\n",
       " 'you',\n",
       " 'smell',\n",
       " 'them',\n",
       " 'Do',\n",
       " 'your',\n",
       " 'old',\n",
       " 'speaker',\n",
       " 'buzz',\n",
       " 'before',\n",
       " 'you',\n",
       " 'get',\n",
       " 'a',\n",
       " 'text',\n",
       " 'Why',\n",
       " 'do',\n",
       " 'people',\n",
       " 'think',\n",
       " 'marijuana',\n",
       " 'should',\n",
       " 'be',\n",
       " 'legalized',\n",
       " 'In',\n",
       " 'the',\n",
       " 'US',\n",
       " 'can',\n",
       " 'your',\n",
       " 'employer',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'on',\n",
       " 'the',\n",
       " 'clock',\n",
       " 'not',\n",
       " 'to',\n",
       " 'join',\n",
       " 'a',\n",
       " 'union',\n",
       " 'Who',\n",
       " 'were',\n",
       " 'some',\n",
       " 'bassist',\n",
       " 'who',\n",
       " 'played',\n",
       " 'with',\n",
       " 'their',\n",
       " 'string',\n",
       " 'in',\n",
       " 'the',\n",
       " 'opposite',\n",
       " 'way',\n",
       " 'they',\n",
       " 're',\n",
       " 'usually',\n",
       " 'arranged',\n",
       " 'Why',\n",
       " 'doe',\n",
       " 'my',\n",
       " 'penis',\n",
       " 'curve',\n",
       " 'one',\n",
       " 'way',\n",
       " 'more',\n",
       " 'than',\n",
       " 'the',\n",
       " 'other',\n",
       " 'Why',\n",
       " 'is',\n",
       " 'reading',\n",
       " 'ok',\n",
       " 'but',\n",
       " 'watching',\n",
       " 'TV',\n",
       " 'movie',\n",
       " 'isn',\n",
       " 't',\n",
       " 'Why',\n",
       " 'doe',\n",
       " 'it',\n",
       " 'feel',\n",
       " 'better',\n",
       " 'to',\n",
       " 'study',\n",
       " 'in',\n",
       " 'bed',\n",
       " 'than',\n",
       " 'a',\n",
       " 'chair',\n",
       " 'desk',\n",
       " 'I',\n",
       " 've',\n",
       " 'heard',\n",
       " 'studying',\n",
       " 'in',\n",
       " 'bed',\n",
       " 'is',\n",
       " 'worse',\n",
       " 'but',\n",
       " 'my',\n",
       " 'comfort',\n",
       " 'say',\n",
       " 'otherwise',\n",
       " 'If',\n",
       " 'you',\n",
       " 'pay',\n",
       " 'cash',\n",
       " 'for',\n",
       " 'gas',\n",
       " 'at',\n",
       " 'a',\n",
       " 'gas',\n",
       " 'station',\n",
       " 'and',\n",
       " 'fill',\n",
       " 'your',\n",
       " 'tank',\n",
       " 'but',\n",
       " 'there',\n",
       " 's',\n",
       " 'still',\n",
       " 'money',\n",
       " 'left',\n",
       " 'what',\n",
       " 'do',\n",
       " 'you',\n",
       " 'do',\n",
       " 'Why',\n",
       " 'do',\n",
       " 'integrated',\n",
       " 'graphic',\n",
       " 'perform',\n",
       " 'worse',\n",
       " 'when',\n",
       " 'console',\n",
       " 'with',\n",
       " 'integrated',\n",
       " 'graphic',\n",
       " 'can',\n",
       " 'handle',\n",
       " 'the',\n",
       " 'latest',\n",
       " 'game',\n",
       " 'Why',\n",
       " 'doe',\n",
       " 'is',\n",
       " 'seem',\n",
       " 'like',\n",
       " 'broadband',\n",
       " 'internet',\n",
       " 'from',\n",
       " '15',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'wa',\n",
       " 'better',\n",
       " 'than',\n",
       " 'what',\n",
       " 'we',\n",
       " 'have',\n",
       " 'today',\n",
       " 'What',\n",
       " 'doe',\n",
       " 'punk',\n",
       " 'mean',\n",
       " 'in',\n",
       " 'Cyberpunk',\n",
       " 'Steampunk',\n",
       " 'Gorepunk',\n",
       " 'etc',\n",
       " 'Is',\n",
       " 'it',\n",
       " 'related',\n",
       " 'to',\n",
       " 'the',\n",
       " 'music',\n",
       " 'genre',\n",
       " 'Threesome',\n",
       " 'Why',\n",
       " 'are',\n",
       " 'people',\n",
       " 'so',\n",
       " 'loyal',\n",
       " 'to',\n",
       " 'sport',\n",
       " 'team',\n",
       " 'When',\n",
       " 'will',\n",
       " 'an',\n",
       " 'advanced',\n",
       " 'A',\n",
       " 'I',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'disobey',\n",
       " 'it',\n",
       " 's',\n",
       " 'master',\n",
       " 'Is',\n",
       " 'it',\n",
       " 'ok',\n",
       " 'to',\n",
       " 'listen',\n",
       " 'to',\n",
       " 'R',\n",
       " 'Kelly',\n",
       " 'again',\n",
       " 'Why',\n",
       " 'do',\n",
       " 'men',\n",
       " 'feel',\n",
       " 'ashamed',\n",
       " 'after',\n",
       " 'masturbating',\n",
       " 'Is',\n",
       " 'it',\n",
       " 'just',\n",
       " 'me',\n",
       " 'or',\n",
       " 'do',\n",
       " 'you',\n",
       " 'all',\n",
       " 'think',\n",
       " 'that',\n",
       " 'we',\n",
       " 'were',\n",
       " 'happy',\n",
       " 'when',\n",
       " 'when',\n",
       " 'were',\n",
       " 'cringe',\n",
       " 'Are',\n",
       " 'there',\n",
       " 'any',\n",
       " 'medication',\n",
       " 'that',\n",
       " 'doctor',\n",
       " 'prescribe',\n",
       " 'that',\n",
       " 'have',\n",
       " 'very',\n",
       " 'harmful',\n",
       " 'side',\n",
       " 'effect',\n",
       " 'or',\n",
       " 'cause',\n",
       " 'serious',\n",
       " 'serious',\n",
       " 'health',\n",
       " 'problem',\n",
       " 'Why',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'Ben',\n",
       " 'Shapiro',\n",
       " 'run',\n",
       " 'for',\n",
       " 'President',\n",
       " 'of',\n",
       " 'the',\n",
       " 'US',\n",
       " 'How',\n",
       " 'are',\n",
       " 'we',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'influence',\n",
       " 'politician',\n",
       " 'about',\n",
       " 'climate',\n",
       " 'change',\n",
       " 'If',\n",
       " 'I',\n",
       " 'have',\n",
       " 'a',\n",
       " 'nose',\n",
       " 'fetish',\n",
       " 'that',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'apply',\n",
       " 'to',\n",
       " 'people',\n",
       " 'could',\n",
       " 'I',\n",
       " 'be',\n",
       " 'a',\n",
       " 'zoophile',\n",
       " 'Why',\n",
       " 'do',\n",
       " 'lawyer',\n",
       " 'always',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'comfort',\n",
       " 'their',\n",
       " 'guilty',\n",
       " 'client',\n",
       " 'Just',\n",
       " 'asking',\n",
       " 'in',\n",
       " 'case',\n",
       " 'anyone',\n",
       " 'ha',\n",
       " 'inpatient',\n",
       " 'psych',\n",
       " 'experience',\n",
       " 'how',\n",
       " 'long',\n",
       " 'until',\n",
       " 'someone',\n",
       " 'can',\n",
       " 'call',\n",
       " 'home',\n",
       " 'when',\n",
       " 'theyre',\n",
       " 'in',\n",
       " 'inpatient',\n",
       " 'psych',\n",
       " 'I',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'I',\n",
       " 'dont',\n",
       " 'deserve',\n",
       " 'my',\n",
       " 'parentsWhats',\n",
       " 'the',\n",
       " 'best',\n",
       " 'way',\n",
       " 'to',\n",
       " 'fatten',\n",
       " 'up',\n",
       " 'a',\n",
       " 'cat',\n",
       " 'certain',\n",
       " 'food',\n",
       " 'activity',\n",
       " 'etc',\n",
       " 'Is',\n",
       " 'plant',\n",
       " 'based',\n",
       " 'meat',\n",
       " 'more',\n",
       " 'healthy',\n",
       " 'than',\n",
       " 'actual',\n",
       " 'meat',\n",
       " 'Why',\n",
       " 'doe',\n",
       " 'all',\n",
       " 'my',\n",
       " 'tech',\n",
       " 'break',\n",
       " 'Why',\n",
       " 'is',\n",
       " 'horror',\n",
       " 'scarier',\n",
       " 'when',\n",
       " 'you',\n",
       " 're',\n",
       " 'tired',\n",
       " 'Is',\n",
       " 'autism',\n",
       " 'a',\n",
       " 'dealbreaker',\n",
       " 'for',\n",
       " 'you',\n",
       " 'If',\n",
       " 'so',\n",
       " 'why',\n",
       " 'I',\n",
       " 'don',\n",
       " 't',\n",
       " 'eat',\n",
       " 'chocolate',\n",
       " 'but',\n",
       " 'when',\n",
       " 'depressed',\n",
       " 'I',\n",
       " 'crave',\n",
       " 'it',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytokenizing_lem(both['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ceac71-3150-4221-a322-f5ce3652e29c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tokenizing-Stemming Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebcfa909-cd22-4cc6-882f-0a38a24f2274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mytokenizing_stem(column):\n",
    "    words = ''\n",
    "    for elements in column:\n",
    "        words += elements\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words_tokens = tokenizer.tokenize(words)\n",
    "    p_stemmer = PorterStemmer()\n",
    "    words_tokens_stem = [p_stemmer.stem(token) for token in words_tokens]\n",
    "    return words_tokens_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12670699-d62f-45b2-81c3-84b362f3cf84",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 's',\n",
       " 'the',\n",
       " 'differ',\n",
       " 'between',\n",
       " 'a',\n",
       " 'dog',\n",
       " 'and',\n",
       " 'a',\n",
       " 'cat',\n",
       " 'lick',\n",
       " 'whi',\n",
       " 'is',\n",
       " 'it',\n",
       " 'wierd',\n",
       " 'to',\n",
       " 'find',\n",
       " 'a',\n",
       " 'latin',\n",
       " 'woman',\n",
       " 'or',\n",
       " 'a',\n",
       " 'latina',\n",
       " 'attract',\n",
       " 'what',\n",
       " 's',\n",
       " 'a',\n",
       " 'healthi',\n",
       " 'way',\n",
       " 'to',\n",
       " 'disciplin',\n",
       " 'myself',\n",
       " 'appli',\n",
       " 'lube',\n",
       " 'whi',\n",
       " 'don',\n",
       " 't',\n",
       " 'the',\n",
       " 'democrat',\n",
       " 'make',\n",
       " 'the',\n",
       " 'republican',\n",
       " 'actual',\n",
       " 'carri',\n",
       " 'out',\n",
       " 'a',\n",
       " 'filibust',\n",
       " 'when',\n",
       " 'discuss',\n",
       " 'suicid',\n",
       " 'and',\n",
       " 'mental',\n",
       " 'health',\n",
       " 'whi',\n",
       " 'doe',\n",
       " 'no',\n",
       " 'one',\n",
       " 'mention',\n",
       " 'that',\n",
       " 'women',\n",
       " 'attempt',\n",
       " 'suicid',\n",
       " '3x',\n",
       " 's',\n",
       " 'the',\n",
       " 'rate',\n",
       " 'that',\n",
       " 'men',\n",
       " 'do',\n",
       " 'what',\n",
       " 'are',\n",
       " 'the',\n",
       " 'most',\n",
       " 'posit',\n",
       " 'subreddit',\n",
       " 'out',\n",
       " 'there',\n",
       " 'is',\n",
       " 'it',\n",
       " 'a',\n",
       " 'normal',\n",
       " 'feel',\n",
       " 'to',\n",
       " 'be',\n",
       " 'confus',\n",
       " 'whi',\n",
       " 'someon',\n",
       " 'you',\n",
       " 'find',\n",
       " 'attract',\n",
       " 'find',\n",
       " 'you',\n",
       " 'attract',\n",
       " 'how',\n",
       " 'do',\n",
       " 'i',\n",
       " 'make',\n",
       " 'digit',\n",
       " 'estim',\n",
       " 'of',\n",
       " 'wood',\n",
       " 'project',\n",
       " 'to',\n",
       " 'get',\n",
       " 'accur',\n",
       " 'measur',\n",
       " 'dirt',\n",
       " 'keep',\n",
       " 'get',\n",
       " 'under',\n",
       " 'my',\n",
       " 'extrem',\n",
       " 'short',\n",
       " 'nail',\n",
       " 'make',\n",
       " 'it',\n",
       " 'pain',\n",
       " 'and',\n",
       " 'difficult',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'clean',\n",
       " 'how',\n",
       " 'can',\n",
       " 'i',\n",
       " 'fix',\n",
       " 'thi',\n",
       " 'what',\n",
       " 'non',\n",
       " 'academ',\n",
       " 'lesson',\n",
       " 'did',\n",
       " 'school',\n",
       " 'drill',\n",
       " 'into',\n",
       " 'you',\n",
       " 'as',\n",
       " 'a',\n",
       " 'kid',\n",
       " 'forget',\n",
       " 'your',\n",
       " 'parent',\n",
       " 'nativ',\n",
       " 'languag',\n",
       " 'ani',\n",
       " 'help',\n",
       " 'advic',\n",
       " 'what',\n",
       " 'do',\n",
       " 'peopl',\n",
       " 'who',\n",
       " 'are',\n",
       " 'kind',\n",
       " 'but',\n",
       " 'also',\n",
       " 'new',\n",
       " 'to',\n",
       " 'me',\n",
       " 'sign',\n",
       " 'xo',\n",
       " 'or',\n",
       " 'xx',\n",
       " 'in',\n",
       " 'their',\n",
       " 'signatur',\n",
       " 'mean',\n",
       " 'what',\n",
       " 'make',\n",
       " 'a',\n",
       " 'convers',\n",
       " 'deep',\n",
       " 'meaning',\n",
       " 'to',\n",
       " 'you',\n",
       " 'do',\n",
       " 'canadian',\n",
       " 'find',\n",
       " 'eh',\n",
       " 'sorri',\n",
       " 'mapl',\n",
       " 'syrup',\n",
       " 'type',\n",
       " 'comment',\n",
       " 'offens',\n",
       " 'whi',\n",
       " 'do',\n",
       " 'american',\n",
       " 'eat',\n",
       " 'so',\n",
       " 'much',\n",
       " 'on',\n",
       " 'thanksgiv',\n",
       " 'if',\n",
       " 'i',\n",
       " 'stil',\n",
       " 'own',\n",
       " 'my',\n",
       " 'own',\n",
       " 'nude',\n",
       " 'tape',\n",
       " 'from',\n",
       " 'when',\n",
       " 'i',\n",
       " 'wa',\n",
       " 'underag',\n",
       " 'can',\n",
       " 'i',\n",
       " 'be',\n",
       " 'in',\n",
       " 'jail',\n",
       " 'for',\n",
       " 'own',\n",
       " 'cp',\n",
       " 'whi',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'ray',\n",
       " 'lewi',\n",
       " 'get',\n",
       " 'the',\n",
       " 'same',\n",
       " 'treatment',\n",
       " 'as',\n",
       " 'oj',\n",
       " 'simpson',\n",
       " 'and',\n",
       " 'or',\n",
       " 'aaron',\n",
       " 'hernandezi',\n",
       " 'it',\n",
       " 'actual',\n",
       " 'that',\n",
       " 'unusu',\n",
       " 'that',\n",
       " 'my',\n",
       " 'mom',\n",
       " 'use',\n",
       " 'to',\n",
       " 'use',\n",
       " 'my',\n",
       " 'hair',\n",
       " 'to',\n",
       " 'decor',\n",
       " 'bread',\n",
       " 'are',\n",
       " 'autist',\n",
       " 'peopl',\n",
       " 'more',\n",
       " 'like',\n",
       " 'to',\n",
       " 'commit',\n",
       " 'sexual',\n",
       " 'crime',\n",
       " 'is',\n",
       " 'male',\n",
       " 'sperm',\n",
       " 'consid',\n",
       " 'a',\n",
       " 'seed',\n",
       " 'is',\n",
       " 'my',\n",
       " 'cpu',\n",
       " 'go',\n",
       " 'to',\n",
       " 'be',\n",
       " 'okay',\n",
       " 'i',\n",
       " 'spill',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'water',\n",
       " 'on',\n",
       " 'my',\n",
       " 'game',\n",
       " 'laptop',\n",
       " '20hr',\n",
       " 'ago',\n",
       " 'immedi',\n",
       " 'turn',\n",
       " 'it',\n",
       " 'upsid',\n",
       " 'down',\n",
       " 'and',\n",
       " 'wipe',\n",
       " 'it',\n",
       " 'down',\n",
       " 'with',\n",
       " 'fabric',\n",
       " 'it',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'still',\n",
       " 'be',\n",
       " 'run',\n",
       " 'but',\n",
       " 'now',\n",
       " 'everi',\n",
       " 'time',\n",
       " 'it',\n",
       " 'stall',\n",
       " 'or',\n",
       " 'anyth',\n",
       " 'i',\n",
       " 'm',\n",
       " 'go',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'myself',\n",
       " 'that',\n",
       " 'marvel',\n",
       " 'urkellian',\n",
       " 'question',\n",
       " 'did',\n",
       " 'i',\n",
       " 'do',\n",
       " 'that',\n",
       " 'whi',\n",
       " 'doe',\n",
       " 'it',\n",
       " 'feel',\n",
       " 'weird',\n",
       " 'walk',\n",
       " 'on',\n",
       " 'a',\n",
       " 'broken',\n",
       " 'escal',\n",
       " 'if',\n",
       " 'i',\n",
       " 'am',\n",
       " 'look',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'hous',\n",
       " 'that',\n",
       " 'is',\n",
       " 'around',\n",
       " '450',\n",
       " '000',\n",
       " 'how',\n",
       " 'much',\n",
       " 'should',\n",
       " 'i',\n",
       " 'put',\n",
       " 'down',\n",
       " 'if',\n",
       " 'i',\n",
       " 'were',\n",
       " 'to',\n",
       " 'somehow',\n",
       " 'pull',\n",
       " 'a',\n",
       " 'fire',\n",
       " 'pin',\n",
       " 'out',\n",
       " 'of',\n",
       " 'a',\n",
       " 'firearm',\n",
       " 'and',\n",
       " 'tap',\n",
       " 'a',\n",
       " 'singular',\n",
       " 'bullet',\n",
       " 'in',\n",
       " 'the',\n",
       " 'place',\n",
       " 'the',\n",
       " 'caus',\n",
       " 'it',\n",
       " 'to',\n",
       " 'ignit',\n",
       " 'could',\n",
       " 'a',\n",
       " 'fire',\n",
       " 'a',\n",
       " 'round',\n",
       " 'without',\n",
       " 'use',\n",
       " 'a',\n",
       " 'gun',\n",
       " 'respons',\n",
       " 'to',\n",
       " 'get',\n",
       " 'drink',\n",
       " 'sent',\n",
       " 'to',\n",
       " 'you',\n",
       " 'by',\n",
       " 'guy',\n",
       " 'wa',\n",
       " 'hitler',\n",
       " 'jewish',\n",
       " 'what',\n",
       " 'doe',\n",
       " 'natto',\n",
       " 'or',\n",
       " 'ferment',\n",
       " 'soy',\n",
       " 'bean',\n",
       " 'actual',\n",
       " 'tast',\n",
       " 'like',\n",
       " 'and',\n",
       " 'is',\n",
       " 'it',\n",
       " 'realli',\n",
       " 'as',\n",
       " 'bad',\n",
       " 'as',\n",
       " 'peopl',\n",
       " 'say',\n",
       " 'it',\n",
       " 'is',\n",
       " 'if',\n",
       " 'someon',\n",
       " 'is',\n",
       " 'alway',\n",
       " 'chang',\n",
       " 'their',\n",
       " 'profil',\n",
       " 'pic',\n",
       " 'doe',\n",
       " 'that',\n",
       " 'mean',\n",
       " 'their',\n",
       " 'not',\n",
       " 'realli',\n",
       " 'secur',\n",
       " 'about',\n",
       " 'how',\n",
       " 'they',\n",
       " 'look',\n",
       " 'how',\n",
       " 'do',\n",
       " 'you',\n",
       " 'express',\n",
       " 'your',\n",
       " 'sexual',\n",
       " 'whi',\n",
       " 'do',\n",
       " 'you',\n",
       " 'think',\n",
       " 'drug',\n",
       " 'paraphernalia',\n",
       " 'is',\n",
       " 'treat',\n",
       " 'so',\n",
       " 'much',\n",
       " 'more',\n",
       " 'harshli',\n",
       " 'than',\n",
       " 'mere',\n",
       " 'drug',\n",
       " 'possess',\n",
       " 'in',\n",
       " 'hawaii',\n",
       " 'i',\n",
       " 'am',\n",
       " 'new',\n",
       " 'on',\n",
       " 'reddit',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'not',\n",
       " 'abl',\n",
       " 'to',\n",
       " 'post',\n",
       " 'question',\n",
       " 'on',\n",
       " 'mani',\n",
       " 'famou',\n",
       " 'subreddit',\n",
       " 'eg',\n",
       " 'r',\n",
       " 'teenag',\n",
       " 'it',\n",
       " 'is',\n",
       " 'show',\n",
       " 'you',\n",
       " 'have',\n",
       " 'low',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'karma',\n",
       " 'can',\n",
       " 'you',\n",
       " 'pleas',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'what',\n",
       " 'should',\n",
       " 'i',\n",
       " 'do',\n",
       " 'when',\n",
       " 'state',\n",
       " 'a',\n",
       " 'pronoun',\n",
       " 'whi',\n",
       " 'do',\n",
       " 'peopl',\n",
       " 'feel',\n",
       " 'the',\n",
       " 'need',\n",
       " 'to',\n",
       " 'list',\n",
       " 'it',\n",
       " 'grammat',\n",
       " 'form',\n",
       " 'whi',\n",
       " 'do',\n",
       " 'peopl',\n",
       " 'treat',\n",
       " 'peopl',\n",
       " 'with',\n",
       " 'disabl',\n",
       " 'like',\n",
       " 'babi',\n",
       " 'even',\n",
       " 'when',\n",
       " 'they',\n",
       " 'are',\n",
       " 'manag',\n",
       " 'etc',\n",
       " 'if',\n",
       " 'i',\n",
       " 'start',\n",
       " 'dress',\n",
       " 'super',\n",
       " 'warm',\n",
       " 'in',\n",
       " 'earli',\n",
       " 'winter',\n",
       " 'will',\n",
       " 'i',\n",
       " 'get',\n",
       " 'use',\n",
       " 'to',\n",
       " 'the',\n",
       " 'cloth',\n",
       " 'and',\n",
       " 'feel',\n",
       " 'colder',\n",
       " 'in',\n",
       " 'late',\n",
       " 'winter',\n",
       " 'am',\n",
       " 'i',\n",
       " 'ci',\n",
       " 'or',\n",
       " 'not',\n",
       " 'do',\n",
       " 'trailer',\n",
       " 'gener',\n",
       " 'come',\n",
       " 'with',\n",
       " 'inverterswhi',\n",
       " 'do',\n",
       " 'human',\n",
       " 'get',\n",
       " 'fat',\n",
       " 'after',\n",
       " 'pregnanc',\n",
       " 'but',\n",
       " 'anim',\n",
       " 'don',\n",
       " 't',\n",
       " 'where',\n",
       " 'can',\n",
       " 'i',\n",
       " 'find',\n",
       " 'a',\n",
       " 'countri',\n",
       " 'ethnic',\n",
       " 'eggsi',\n",
       " 'there',\n",
       " 'such',\n",
       " 'thing',\n",
       " 'as',\n",
       " 'a',\n",
       " 'good',\n",
       " 'dick',\n",
       " 'pic',\n",
       " 'if',\n",
       " 'so',\n",
       " 'how',\n",
       " 'do',\n",
       " 'you',\n",
       " 'do',\n",
       " 'such',\n",
       " 'a',\n",
       " 'thing',\n",
       " 'prepar',\n",
       " 'for',\n",
       " 'a',\n",
       " 'big',\n",
       " 'move',\n",
       " 'out',\n",
       " 'of',\n",
       " 'state',\n",
       " 'how',\n",
       " 'do',\n",
       " 'adult',\n",
       " 'make',\n",
       " 'new',\n",
       " 'friend',\n",
       " 'doe',\n",
       " 'anyon',\n",
       " 'els',\n",
       " 'feel',\n",
       " 'an',\n",
       " 'overwhelm',\n",
       " 'urg',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'tenni',\n",
       " 'ball',\n",
       " 'when',\n",
       " 'you',\n",
       " 'smell',\n",
       " 'them',\n",
       " 'do',\n",
       " 'your',\n",
       " 'old',\n",
       " 'speaker',\n",
       " 'buzz',\n",
       " 'befor',\n",
       " 'you',\n",
       " 'get',\n",
       " 'a',\n",
       " 'text',\n",
       " 'whi',\n",
       " 'do',\n",
       " 'peopl',\n",
       " 'think',\n",
       " 'marijuana',\n",
       " 'should',\n",
       " 'be',\n",
       " 'legal',\n",
       " 'in',\n",
       " 'the',\n",
       " 'us',\n",
       " 'can',\n",
       " 'your',\n",
       " 'employ',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'on',\n",
       " 'the',\n",
       " 'clock',\n",
       " 'not',\n",
       " 'to',\n",
       " 'join',\n",
       " 'a',\n",
       " 'union',\n",
       " 'who',\n",
       " 'were',\n",
       " 'some',\n",
       " 'bassist',\n",
       " 'who',\n",
       " 'play',\n",
       " 'with',\n",
       " 'their',\n",
       " 'string',\n",
       " 'in',\n",
       " 'the',\n",
       " 'opposit',\n",
       " 'way',\n",
       " 'they',\n",
       " 're',\n",
       " 'usual',\n",
       " 'arrang',\n",
       " 'whi',\n",
       " 'doe',\n",
       " 'my',\n",
       " 'peni',\n",
       " 'curv',\n",
       " 'one',\n",
       " 'way',\n",
       " 'more',\n",
       " 'than',\n",
       " 'the',\n",
       " 'other',\n",
       " 'whi',\n",
       " 'is',\n",
       " 'read',\n",
       " 'ok',\n",
       " 'but',\n",
       " 'watch',\n",
       " 'tv',\n",
       " 'movi',\n",
       " 'isn',\n",
       " 't',\n",
       " 'whi',\n",
       " 'doe',\n",
       " 'it',\n",
       " 'feel',\n",
       " 'better',\n",
       " 'to',\n",
       " 'studi',\n",
       " 'in',\n",
       " 'bed',\n",
       " 'than',\n",
       " 'a',\n",
       " 'chair',\n",
       " 'desk',\n",
       " 'i',\n",
       " 've',\n",
       " 'heard',\n",
       " 'studi',\n",
       " 'in',\n",
       " 'bed',\n",
       " 'is',\n",
       " 'wors',\n",
       " 'but',\n",
       " 'my',\n",
       " 'comfort',\n",
       " 'say',\n",
       " 'otherwis',\n",
       " 'if',\n",
       " 'you',\n",
       " 'pay',\n",
       " 'cash',\n",
       " 'for',\n",
       " 'ga',\n",
       " 'at',\n",
       " 'a',\n",
       " 'ga',\n",
       " 'station',\n",
       " 'and',\n",
       " 'fill',\n",
       " 'your',\n",
       " 'tank',\n",
       " 'but',\n",
       " 'there',\n",
       " 's',\n",
       " 'still',\n",
       " 'money',\n",
       " 'left',\n",
       " 'what',\n",
       " 'do',\n",
       " 'you',\n",
       " 'do',\n",
       " 'whi',\n",
       " 'do',\n",
       " 'integr',\n",
       " 'graphic',\n",
       " 'perform',\n",
       " 'wors',\n",
       " 'when',\n",
       " 'consol',\n",
       " 'with',\n",
       " 'integr',\n",
       " 'graphic',\n",
       " 'can',\n",
       " 'handl',\n",
       " 'the',\n",
       " 'latest',\n",
       " 'game',\n",
       " 'whi',\n",
       " 'doe',\n",
       " 'is',\n",
       " 'seem',\n",
       " 'like',\n",
       " 'broadband',\n",
       " 'internet',\n",
       " 'from',\n",
       " '15',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'wa',\n",
       " 'better',\n",
       " 'than',\n",
       " 'what',\n",
       " 'we',\n",
       " 'have',\n",
       " 'today',\n",
       " 'what',\n",
       " 'doe',\n",
       " 'punk',\n",
       " 'mean',\n",
       " 'in',\n",
       " 'cyberpunk',\n",
       " 'steampunk',\n",
       " 'gorepunk',\n",
       " 'etc',\n",
       " 'is',\n",
       " 'it',\n",
       " 'relat',\n",
       " 'to',\n",
       " 'the',\n",
       " 'music',\n",
       " 'genr',\n",
       " 'threesom',\n",
       " 'whi',\n",
       " 'are',\n",
       " 'peopl',\n",
       " 'so',\n",
       " 'loyal',\n",
       " 'to',\n",
       " 'sport',\n",
       " 'team',\n",
       " 'when',\n",
       " 'will',\n",
       " 'an',\n",
       " 'advanc',\n",
       " 'a',\n",
       " 'i',\n",
       " 'be',\n",
       " 'abl',\n",
       " 'to',\n",
       " 'disobey',\n",
       " 'it',\n",
       " 's',\n",
       " 'master',\n",
       " 'is',\n",
       " 'it',\n",
       " 'ok',\n",
       " 'to',\n",
       " 'listen',\n",
       " 'to',\n",
       " 'r',\n",
       " 'kelli',\n",
       " 'again',\n",
       " 'whi',\n",
       " 'do',\n",
       " 'men',\n",
       " 'feel',\n",
       " 'asham',\n",
       " 'after',\n",
       " 'masturb',\n",
       " 'is',\n",
       " 'it',\n",
       " 'just',\n",
       " 'me',\n",
       " 'or',\n",
       " 'do',\n",
       " 'you',\n",
       " 'all',\n",
       " 'think',\n",
       " 'that',\n",
       " 'we',\n",
       " 'were',\n",
       " 'happi',\n",
       " 'when',\n",
       " 'when',\n",
       " 'were',\n",
       " 'cring',\n",
       " 'are',\n",
       " 'there',\n",
       " 'ani',\n",
       " 'medic',\n",
       " 'that',\n",
       " 'doctor',\n",
       " 'prescrib',\n",
       " 'that',\n",
       " 'have',\n",
       " 'veri',\n",
       " 'harm',\n",
       " 'side',\n",
       " 'effect',\n",
       " 'or',\n",
       " 'caus',\n",
       " 'seriou',\n",
       " 'seriou',\n",
       " 'health',\n",
       " 'problem',\n",
       " 'whi',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'ben',\n",
       " 'shapiro',\n",
       " 'run',\n",
       " 'for',\n",
       " 'presid',\n",
       " 'of',\n",
       " 'the',\n",
       " 'us',\n",
       " 'how',\n",
       " 'are',\n",
       " 'we',\n",
       " 'suppos',\n",
       " 'to',\n",
       " 'influenc',\n",
       " 'politician',\n",
       " 'about',\n",
       " 'climat',\n",
       " 'chang',\n",
       " 'if',\n",
       " 'i',\n",
       " 'have',\n",
       " 'a',\n",
       " 'nose',\n",
       " 'fetish',\n",
       " 'that',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'appli',\n",
       " 'to',\n",
       " 'peopl',\n",
       " 'could',\n",
       " 'i',\n",
       " 'be',\n",
       " 'a',\n",
       " 'zoophil',\n",
       " 'whi',\n",
       " 'do',\n",
       " 'lawyer',\n",
       " 'alway',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'comfort',\n",
       " 'their',\n",
       " 'guilti',\n",
       " 'client',\n",
       " 'just',\n",
       " 'ask',\n",
       " 'in',\n",
       " 'case',\n",
       " 'anyon',\n",
       " 'ha',\n",
       " 'inpati',\n",
       " 'psych',\n",
       " 'experi',\n",
       " 'how',\n",
       " 'long',\n",
       " 'until',\n",
       " 'someon',\n",
       " 'can',\n",
       " 'call',\n",
       " 'home',\n",
       " 'when',\n",
       " 'theyr',\n",
       " 'in',\n",
       " 'inpati',\n",
       " 'psych',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'i',\n",
       " 'dont',\n",
       " 'deserv',\n",
       " 'my',\n",
       " 'parentswhat',\n",
       " 'the',\n",
       " 'best',\n",
       " 'way',\n",
       " 'to',\n",
       " 'fatten',\n",
       " 'up',\n",
       " 'a',\n",
       " 'cat',\n",
       " 'certain',\n",
       " 'food',\n",
       " 'activ',\n",
       " 'etc',\n",
       " 'is',\n",
       " 'plant',\n",
       " 'base',\n",
       " 'meat',\n",
       " 'more',\n",
       " 'healthi',\n",
       " 'than',\n",
       " 'actual',\n",
       " 'meat',\n",
       " 'whi',\n",
       " 'doe',\n",
       " 'all',\n",
       " 'my',\n",
       " 'tech',\n",
       " 'break',\n",
       " 'whi',\n",
       " 'is',\n",
       " 'horror',\n",
       " 'scarier',\n",
       " 'when',\n",
       " 'you',\n",
       " 're',\n",
       " 'tire',\n",
       " 'is',\n",
       " 'autism',\n",
       " 'a',\n",
       " 'dealbreak',\n",
       " 'for',\n",
       " 'you',\n",
       " 'if',\n",
       " 'so',\n",
       " 'whi',\n",
       " 'i',\n",
       " 'don',\n",
       " 't',\n",
       " 'eat',\n",
       " 'chocol',\n",
       " 'but',\n",
       " 'when',\n",
       " 'depress',\n",
       " 'i',\n",
       " 'crave',\n",
       " 'it',\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytokenizing_stem(both['title']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c88633-4bf2-4361-a9a0-0d3f5ce74669",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text Edits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28342173-31ac-4ab2-9510-0496d193d33a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8726871c-bd1c-4049-b951-019c2e1c09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make use in a different model; not particularly useful\n",
    "\n",
    "def split_count(array):\n",
    "    emoji_list = []\n",
    "    for text in array:\n",
    "        data = regex.findall(r'\\X', text)\n",
    "        for word in data:\n",
    "            if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "                emoji_list.append(word)\n",
    "    \n",
    "    return emoji_list\n",
    "\n",
    "https://stackoverflow.com/questions/43146528/how-to-extract-all-the-emojis-from-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a24078e8-8f49-4f83-b3b9-762c5c671ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_count(both['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f5860e3b-1021-4c89-9f1c-b327f6eebd18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "852"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_count(both['selftext']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701fc525-33b8-4c13-9734-7867b2fd24a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d34fbaf-fce2-470c-8660-ffa3235660f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"it\\'s\", \"its\", phrase)\n",
    "    phrase = re.sub(r\"What\\'s\", \"What is\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    # phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f30c125-ca6f-4fe3-98e8-1f6375e706f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanding contracted text\n",
    "for col in ['text']:\n",
    "    both['text'] = [decontracted(each) for each in both[col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e57a7cc-c99c-4373-9aae-f336a5ff0f62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad3f72f7-e28b-49a9-b7e3-99ae56cab042",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_stop_words = list(CountVectorizer(stop_words = 'english').get_stop_words())\n",
    "my_stop_words = ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv', 'anywh', 'becau', 'el', 'elsewh', 'everywh', 'ind', 'otherwi', 'plea', 'somewh'] \n",
    "my_stop_words_same = ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] \n",
    "\n",
    "\n",
    "\n",
    "custom_stop_words = default_stop_words + my_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e040bc-ea0b-43d0-8334-5f5bc51cf188",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modeling Pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d30e056b-8d6f-48cd-8b90-76980ad2df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = both['title']\n",
    "# y = both['subreddit_nsq']\n",
    "\n",
    "# X = both['selftext']\n",
    "# y = both['subreddit_nsq']\n",
    "\n",
    "X = both['text']\n",
    "y = both['subreddit_nsq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4acbdc55-d72a-4e81-b274-2255c288ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=24, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d82a30f-4be8-448c-a9ca-6d66ee8efe20",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model dataframe\n",
    "model_df = pd.DataFrame(columns=['Model', 'Train', 'Test'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d9f37-5369-4f51-8ada-81ff15e30a0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Random Forests Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40523a14-e24c-4604-8902-08823f1af21c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### RFC Default/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35044397-3a6b-4dac-9b56-caf91df7dc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9996, 0.6692)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('rfc', RandomForestClassifier()), \n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1760ed65-5201-459e-ba7d-8136a25b31d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9996, 0.6744)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tv', TfidfVectorizer(tokenizer=mytokenizing_stem)), \n",
    "    ('rfc', RandomForestClassifier()), \n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a13131-88f0-46e0-9deb-8ba484aa1de4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### RFC LEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "41c89405-f24b-40f7-9e30-b2ad325ca828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9996, 0.6665)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer(tokenizer=mytokenizing_lem)), \n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df2e5d-297d-4013-bd9f-5f6a0783e3a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### RFC Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d76cc462-0a14-4260-b603-1f77fe6a2415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9996, 0.6726)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer(tokenizer=mytokenizing_stem, stop_words=my_stop_words)), \n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a12579-b79d-419d-bb9b-d829d0f06a24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Insights\n",
    "* Stemming performs the best with RFC\n",
    "* Overfit\n",
    "* Best train test scores .99 .6726"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd0a3a3-434e-47b5-b1d3-4dd94d152343",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "424ca7c1-acf7-4ece-9296-a392a714cdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9996, 0.6793)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer(tokenizer=mytokenizing_stem)), \n",
    "    ('efc', ExtraTreesClassifier(max_features=500))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5b897-deb0-44a8-837a-a423dcce488b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85384933-70da-44e2-b09f-6f29237f7f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7045333333333333, 0.6857)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer(min_df=2, tokenizer=mytokenizing_stem)), \n",
    "    ('logreg', LogisticRegression(C=.003, random_state=24, max_iter=200))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fceda26-6229-48b3-ba62-bc73223788a0",
   "metadata": {},
   "source": [
    "##### Insights\n",
    "* train test scores .7309 .6886"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69ceb4-c97f-4b9f-bb03-2e5d550ed71b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Grid Search Logreg Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f396bee-fc2d-4ce7-8602-58d20efa6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cv__stop_words': [None],\n",
    "    'cv__min_df': [1, 2, 3],\n",
    "    'cv__max_df': [1.0, .95],\n",
    "    'cv__tokenizer': [None, mytokenizing_lem, mytokenizing_stem],\n",
    "    'logreg__C': [.0001, .001, .01, .1, 1, 10],\n",
    "    'logreg__random_state': [24],\n",
    "    'logreg__max_iter': [100, 150, 200, 250],\n",
    "    'logreg__class_weight': [None, 'balanced'],\n",
    "    'logreg__max_iter':[200, 400, 600]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05af60-d8e1-4698-8773-80b7fa98d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe,\n",
    "                  param_grid=pipe_params,\n",
    "                  cv=5)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "gs.best_score_\n",
    "\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f489745b-9035-4257-aaa9-a95aa8bdc3ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Randomized Search Logreg Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "89e93f3d-ee5d-4f3f-a915-1607b3f40b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "RANDOMIZED SEARCH\n",
      "Best score: 0.6796131699911369\n",
      "Best params: {'logreg__random_state': 24, 'logreg__max_iter': 600, 'logreg__class_weight': 'balanced', 'logreg__C': 0.01, 'cv__tokenizer': <function mytokenizing_stem at 0x0000028C07D04EE0>, 'cv__stop_words': None, 'cv__min_df': 2, 'cv__max_df': 0.95}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random search\n",
    "rscv = RandomizedSearchCV(estimator = pipe,\n",
    "                     param_distributions = pipe_params,\n",
    "                     scoring = 'f1_weighted',\n",
    "                     n_iter = 100,\n",
    "                     n_jobs = -2,\n",
    "                     cv = 5,\n",
    "                     verbose = 1)\n",
    "\n",
    "# Fit our model\n",
    "rscv.fit(X_train, y_train)\n",
    "\n",
    "# Results\n",
    "print(\"RANDOMIZED SEARCH\")\n",
    "print(f\"Best score: {rscv.best_score_}\")\n",
    "print(f\"Best params: {rscv.best_params_}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da23924-4241-4704-8d0c-528628ae2ffc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Bayesian Search SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31ff597c-0b41-40db-b256-40f06d454975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmoli\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6807178116482109\n",
      "Best parameters: OrderedDict([('cv__max_df', 3.9426340945991716), ('cv__min_df', 2), ('cv__stop_words', 'english'), ('cv__tokenizer', <function mytokenizing_stem at 0x0000023FAFB77B80>), ('svc__C', 2.782559402207126), ('svc__coef0', 0.014961257124677333), ('svc__degree', 4), ('svc__gamma', 'scale'), ('svc__kernel', 'rbf'), ('svc__shrinking', True)])\n",
      "Wall time: 10h 43min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Same old pipeline\n",
    "bs_pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('svc', SVC(random_state = 24))\n",
    "])\n",
    "\n",
    "# Bayes params -- distributions!\n",
    "bs_params = {\n",
    "    'cv__stop_words': [None, 'english'],\n",
    "    'cv__min_df': [1, 2, 3],\n",
    "    'cv__max_df': [1.0, 5.0],\n",
    "    'cv__tokenizer': [mytokenizing_stem],\n",
    "    'svc__C': np.logspace(-5,2, 10), #Real(1e-5, 1e+2, prior='log-uniform'),  was: loguniform(1e-5,1e+2), # was: np.logspace(-5,2, 10),\n",
    "    'svc__kernel': Categorical(['poly','rbf']),\n",
    "    'svc__gamma': Categorical(['scale','auto']),\n",
    "    'svc__degree': Integer(2,10), # can now sample all integer values freely\n",
    "    'svc__coef0': Real(0,1, prior='uniform'), # was: np.linspace(0,1, 5),\n",
    "    'svc__shrinking': Categorical([True, False])  \n",
    "}\n",
    "\n",
    "# Bayes hyperparameter search\n",
    "svc_bs = BayesSearchCV(estimator = bs_pipe,\n",
    "                     search_spaces = bs_params,\n",
    "                     scoring = 'f1_weighted',\n",
    "                     n_iter = 15,\n",
    "                     cv = 5,\n",
    "                     verbose = 1,\n",
    "                     random_state=24,\n",
    "                     n_jobs=10,\n",
    "                        n_points=2\n",
    "                      )\n",
    "\n",
    "svc_bs.fit(X_train, y_train);\n",
    "print(f\"Best score: {svc_bs.best_score_}\")\n",
    "print(f\"Best parameters: {svc_bs.best_params_}\")\n",
    "\n",
    "# credit to hyperparameters breakfasthour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305686e6-91b9-43cc-8164-cb86a3ecd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when exponentiating makes sure to divide log coefficient by standard deviation (works out in the math)\n",
    "# or can do np.exp(logreg.coeff_/ss.scale_) or np.exp(log odds coefficient)**(1/std dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd54f62-b67c-4c94-9af0-e9cf24a2b622",
   "metadata": {},
   "source": [
    "##### TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "475832a7-d367-401f-b7ff-9f3229cb007c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6607666666666666, 0.6501)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tv', TfidfVectorizer(tokenizer=mytokenizing_stem)), \n",
    "    ('logreg', LogisticRegression(C=.01, random_state=24, max_iter=1000))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b65e97-ddbc-409d-a8f6-394b003f050e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Ada Booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "00857462-54d4-4f59-af54-c3563b87e294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6941333333333334, 0.6744)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('abc', AdaBoostClassifier(random_state=24, n_estimators=250))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6b0fcf-e6ab-429f-b681-49fbb3f57374",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Gradient Booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2b9cf179-469e-4d5c-a950-faeb3f40611f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7074333333333334, 0.6858)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer(tokenizer=mytokenizing_stem)),\n",
    "    ('gb', GradientBoostingClassifier(random_state=24, n_estimators=250))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2973bf1-3966-4f58-91aa-7a6fc3fb8454",
   "metadata": {},
   "source": [
    "##### Insights:\n",
    "* .7045 .6795\n",
    "* .7003 .6839"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9adfbc-0f78-421f-aebe-ee337882afdc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "eec84fbd-74fa-4f83-8e21-7dc8a430b979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmoli\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:13:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7867666666666666, 0.683)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer(tokenizer=mytokenizing_stem)),\n",
    "    ('xgb', XGBClassifier())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0404fa-5ebe-4fe1-9c15-9513ec3f56dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f961cab7-2ff1-47f3-9f24-3e83c59964aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6212333333333333, 0.6114)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer(min_df=2, tokenizer=mytokenizing_stem)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=200))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcaa8ce-eaa7-41cc-808f-269d3322f3bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Multinomial NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d2668bf-f6b6-4829-aafa-035e199c8a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7253333333333334, 0.6886)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer(min_df=2, tokenizer=mytokenizing_stem, max_features=8_000)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "447ad9a8-9bfe-49b8-b82b-6d244300e9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7143, 0.688)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer(min_df=2, tokenizer=mytokenizing_stem, max_features=8_000)),\n",
    "    ('mnb', MultinomialNB(alpha=10))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a07c72-143c-4457-9cce-199440817ba7",
   "metadata": {},
   "source": [
    ".7305 .6912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1299abb-fe3e-4cb6-bc1a-dfd82da2bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/mmoli/GA/projects/project_3/data/pickles/mnb.pkl', 'wb') as pickle_out:\n",
    "    pickle.dump(pipe, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "47102a74-ebe8-4774-a635-4c5adf3073e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7377666666666667, 0.691)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer(min_df=2, tokenizer=mytokenizing_stem)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c257523-3ab9-4ffc-9d76-cd2630136b31",
   "metadata": {},
   "source": [
    ".7377 .691"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538d072-a3e4-4c2d-a16c-565d2ff7feb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c23760c5-f35b-4c90-ab5c-65eef479d1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.85, 0.6859)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer(min_df=2, tokenizer=mytokenizing_stem)),\n",
    "    ('svc', SVC(C=2.78))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68537ad0-6fa5-4f07-8614-fdd4484a746d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3600 candidates, totalling 18000 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gs_pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('svc', SVC(random_state = 24))\n",
    "])\n",
    "\n",
    "gs_params = {\n",
    "    'svc__C': np.logspace(-5,2, 10),\n",
    "    'svc__kernel': ['poly','rbf'],\n",
    "    'svc__gamma': ['scale','auto'],\n",
    "    'svc__degree': np.linspace(2,10,9),\n",
    "    'svc__coef0': np.linspace(0,1, 5),\n",
    "    'svc__shrinking': [True, False],   \n",
    "}\n",
    "\n",
    "svc_gs = GridSearchCV(estimator = gs_pipe,\n",
    "                     param_grid = gs_params,\n",
    "                     scoring = 'f1_weighted',\n",
    "                     cv = 5,\n",
    "                     n_jobs = -2,\n",
    "                     verbose = 1)\n",
    "\n",
    "# Fit model\n",
    "svc_gs.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best score: {svc_gs.best_score_}\")\n",
    "print(f\"Best params: {svc_gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89789b1-21e1-425a-8090-690808d8e8eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rs_pipe = Pipeline([\n",
    "    ('ss', CountVectorizer()),\n",
    "    ('svc', SVC(random_state = 24))\n",
    "])\n",
    "\n",
    "# Hyperparameters -- distributions!\n",
    "rs_params = {\n",
    "    'svc__C': np.logspace (-5, 2, 10), #loguniform(1e-5,1e+2), # was: np.logspace(-5,2, 10),\n",
    "    'svc__kernel': ['poly','rbf'],\n",
    "    'svc__gamma': ['scale','auto'],\n",
    "    'svc__degree': list(np.linspace(2,10,9)), # same as before because we need integers\n",
    "    'svc__shrinking': [True, False],   \n",
    "}\n",
    "\n",
    "# Random search\n",
    "svc_rs = RandomizedSearchCV(estimator = rs_pipe,\n",
    "                     param_distributions = rs_params,\n",
    "                     scoring = 'f1_weighted',\n",
    "                     n_iter = 50,\n",
    "                     n_jobs = -2,\n",
    "                     cv = 5,\n",
    "                     verbose = 1)\n",
    "\n",
    "# Fit our model\n",
    "svc_rs.fit(X_train, y_train)\n",
    "\n",
    "# Results\n",
    "print(\"RANDOMIZED SEARCH\")\n",
    "print(f\"Best score: {svc_rs.best_score_}\")\n",
    "print(f\"Best params: {svc_rs.best_params_}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc6fefa-6af2-4240-b485-87760a6ed441",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Modeling Insights\n",
    "* Models hit a test score limitation between .68 and .69; limitation of data or not enough featuring engineering to emphasize differences between NSQ and TATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82804899-3676-40f9-9068-88c7876ea7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['Model'] = ['Random Forest Classifier', 'Extra Trees Classifier', 'Logisitic Regression', 'Adaptive Booster', 'Gradient Booster', 'Extreme Gradient Booster', 'K-Nearest Neighbors', 'Multinomial Naive Bayes', 'Support Vector Machine']\n",
    "model_df['Train'] = [.9996, .9996, .7045, .6941, .7074, .7867, .6212, .7377, .8500]\n",
    "model_df['Test'] = [.6692, .6793, .6857, .6744, .6858, .6830, .6114, .6910, .6859]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af264245-bba8-4a8c-9f91-e8d069b55134",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = model_df.sort_values(by='Test', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bb3ed158-1f87-448d-bed2-423fb338a615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.7377</td>\n",
       "      <td>0.6910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.6859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Booster</td>\n",
       "      <td>0.7074</td>\n",
       "      <td>0.6858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logisitic Regression</td>\n",
       "      <td>0.7045</td>\n",
       "      <td>0.6857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Extreme Gradient Booster</td>\n",
       "      <td>0.7867</td>\n",
       "      <td>0.6830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.6793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adaptive Booster</td>\n",
       "      <td>0.6941</td>\n",
       "      <td>0.6744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.6692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.6212</td>\n",
       "      <td>0.6114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model   Train    Test\n",
       "7   Multinomial Naive Bayes  0.7377  0.6910\n",
       "8    Support Vector Machine  0.8500  0.6859\n",
       "4          Gradient Booster  0.7074  0.6858\n",
       "2      Logisitic Regression  0.7045  0.6857\n",
       "5  Extreme Gradient Booster  0.7867  0.6830\n",
       "1    Extra Trees Classifier  0.9996  0.6793\n",
       "3          Adaptive Booster  0.6941  0.6744\n",
       "0  Random Forest Classifier  0.9996  0.6692\n",
       "6       K-Nearest Neighbors  0.6212  0.6114"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf7a3ac-e10b-4d3c-8c29-fd705c732508",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aba4a40-3f53-487d-ab8f-3c2e8bdb44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating \n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# scores for each string\n",
    "scores = [sia.polarity_scores(each) for each in both['text']]\n",
    "\n",
    "# df of scores\n",
    "score_total = pd.DataFrame(scores)\n",
    "\n",
    "# merging sia scores with both dataframe\n",
    "both = both.merge(score_total, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a5cca95-e591-436d-ba07-c314f838878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# title word count\n",
    "both['title_word_count'] = both['title'].map(str.split).map(len)\n",
    "\n",
    "# title length\n",
    "both['title_length'] = both['title'].map(len)\n",
    "\n",
    "# selftext word count\n",
    "both['selftext_word_count'] = both['selftext'].map(str.split).map(len)\n",
    "\n",
    "# selftext length\n",
    "both['selftext_length'] = both['selftext'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50c0c52d-7256-4c8b-b8dd-6035dacc463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total word count (least efficient way)\n",
    "word_count = []\n",
    "length_count = []\n",
    "for i in range(0, len(both)):\n",
    "    x = both['title_word_count'][i] + both['selftext_word_count'][i]\n",
    "    word_count.append(x)\n",
    "    y = both['title_length'][i] + both['selftext_length'][i]\n",
    "    length_count.append(y)\n",
    "\n",
    "both['total_word_count'] = word_count\n",
    "\n",
    "both['total_length'] = length_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d54c1857-5351-4050-8ebf-79a456072d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoji function altered for counts\n",
    "def emoji_num_counter(array):\n",
    "    emoji_list = []\n",
    "    count_list = []\n",
    "    for text in array:\n",
    "        data = regex.findall(r'\\X', text)\n",
    "        for word in data:\n",
    "            emoji_counter = 0\n",
    "            if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "                emoji_list.append(word)\n",
    "                emoji_counter +=1\n",
    "            count_list.append(emoji_counter)\n",
    "    return sum(count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9ea34ff-135e-4781-93ca-c667dd6ed31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoji count column\n",
    "for col in ['text']:\n",
    "    new_list = []\n",
    "    for each in both['text']:\n",
    "        new_list.append(emoji_num_counter(each))\n",
    "\n",
    "both['emoji_count'] = new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc2ab6-13e2-4d81-be2b-3023b7011c78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Logistic Regression Using Non-NLP Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac8d3a47-f851-4d27-b28b-dc225df06c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_nsq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subreddit_nsq</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_utc</th>\n",
       "      <td>0.622564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>-0.138920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neu</th>\n",
       "      <td>0.153235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>-0.059483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compound</th>\n",
       "      <td>0.052739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_word_count</th>\n",
       "      <td>-0.002188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_length</th>\n",
       "      <td>-0.002417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selftext_word_count</th>\n",
       "      <td>-0.140096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selftext_length</th>\n",
       "      <td>-0.136358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_word_count</th>\n",
       "      <td>-0.139346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_length</th>\n",
       "      <td>-0.135538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emoji_count</th>\n",
       "      <td>-0.008858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     subreddit_nsq\n",
       "subreddit_nsq             1.000000\n",
       "created_utc               0.622564\n",
       "neg                      -0.138920\n",
       "neu                       0.153235\n",
       "pos                      -0.059483\n",
       "compound                  0.052739\n",
       "title_word_count         -0.002188\n",
       "title_length             -0.002417\n",
       "selftext_word_count      -0.140096\n",
       "selftext_length          -0.136358\n",
       "total_word_count         -0.139346\n",
       "total_length             -0.135538\n",
       "emoji_count              -0.008858"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation of non nlp features\n",
    "both.drop(columns=['title', 'selftext', 'text']).corr()[['subreddit_nsq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "1e271533-2c2f-4a69-a13f-cc175e75f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e40a71-6b11-47ec-893c-bac4751fb5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = both[['created_utc', 'neg', 'neu', 'compound', 'total_word_count', 'total_length']]\n",
    "y = both['subreddit_nsq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "68c75b03-fd29-4990-acfb-37602a637507",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=24, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "9ca67aa9-8a4e-42db-a09a-84e43d8446d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8684333333333333, 0.8573)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('ms', StandardScaler()),\n",
    "    ('etc', GradientBoostingClassifier(random_state=24, n_estimators=250))\n",
    "]) \n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6b93f-db48-4182-8d8f-9ba43ccd1d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models capping off at the same spot --- indication that that's as good as it gets\n",
    "# Next steps:\n",
    "# confusion matrix: where are the errors? how to optimize for that--look at distribution\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc429460-dfd5-463c-a524-b17784ca3849",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Insights:\n",
    "* Surprisingly created_utc has the best correlation with subreddits\n",
    "* Sia has limited value to modeling and can be excluded\n",
    "* The model above reflects that classification for this data set is better and more efficiently achieved by using utc than the actual text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
